
<!DOCTYPE html>
<html>
<title>Gesture Link</title>
<body>


       <nav class="navbar navbar-default" role="navigation">
    	  <div class="container">
		    <!-- Brand and toggle get grouped for better mobile display -->
		    <div class="navbar-header">
		      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-brand-centered">
		        <span class="sr-only">Toggle navigation</span>
		        <span class="icon-bar"></span>
		        <span class="icon-bar"></span>
		        <span class="icon-bar"></span>
		      </button>
		      <a href="">
		      <div class="navbar-brand navbar-brand-centered" href=""   style="font-style: bold; font-size: 20px;">Gesture Link</div></a>
		    </div>

		    <!-- Collect the nav links, forms, and other content for toggling -->
		    <div class="collapse navbar-collapse" id="navbar-brand-centered">
		      <ul class="nav navbar-nav">
		        <li class="active"><a href="proposal">Proposal</a></li>
		        <li><a href="slides">Slides</a></li>
		      </ul>
		      <ul class="nav navbar-nav navbar-right">
		        <li><a href="demo">Demo</a></li>
		      </ul>
		    </div><!-- /.navbar-collapse -->
		  </div><!-- /.container-fluid -->
		</nav>

<style type="text/css">
h1{
	text-align: center;
}

@media screen and (min-width:768px){
    .navbar-brand-centered {

        position: absolute;
        left: 45%;
        display: block;
        width: 160px;
        text-align: center;
        background-color: #eee;
    }
    .navbar>.container .navbar-brand-centered,
    .navbar>.container-fluid .navbar-brand-centered {
        margin-left: -80px;
    }
}
</style>

	<div style="max-width:750px;" class="container">
	<center><img style="width:750px;" src="http://deskofkyle.com/hci-project/sid1.png"></center></img>
	<h1>Team: Gesture Link</h1>

	<h1>Problem Statement </h1>
<p>Gesture interaction with computers has largely been limited by the availability, cost, and size of input devices. For instance, mobile phones use capacitive touch screens to render physical taps and gestures as input to the computer. If the size of the touch-area needed was to be bigger, the cost of building such a device would be a function of its screen size (very expensive). Since many people want to interact with computers in their home, office, or personal space, there is no way for a computer to obtain touch or gesture information unless it is performed on an expensive capacitive touch screen like a mobile phone. For instance, “smart” light bulbs, “smart” door locks, and music players need to be controlled by a touch screen. There is no way to obtain a human-computer interface without carrying a device with you to accomplish these tasks. The main idea: touch and gesture input into large surfaces for the means of establishing a human-computer interface is too costly, small, and limited today. There should be a way for a computer to receive input (tap and gestures) without having to do these gestures on a tiny touch screen. </p>

<h1>Proposed Solution</h1>
<p>When two surfaces interact with each other, they produce a unique sound that can be captured with a microphone connected to the computer. Even on large surfaces like walls and tables, an interaction between two objects (i.e. a finger and a table) can be recorded by a microphone and computer. The unique motion of the finger on the table creates a unique signal waveform on the computer. For example, an “X” writing on the table would produce a different signature than an “O” shape. Since this interaction can be recorded with high-quality microphones that block out low frequency noise, an effective “scratch input” device can be built to create a human-computer interface for large surfaces like walls and tables.

This input on walls and surfaces could control “smart” lights, “smart” door locks, and music controls among other things. In a “smart home” which usually relied on a highly accurate touch screen to be carried at all times, a passive gesture on a wall with a system like this could signal to the house to perform certain actions. Since the scratch input can be heard on many different surfaces, it opens up the possibility for human-computer interfaces to exists on extremely large surfaces, instead of tiny touch screens that have been used up until this point.

The proposed solution would be a microphone that is specially tuned to listen to low-frequency noise on a table or wall. This device would be placed on that surface, be connected to a computer program, and could interpret gestures that it hears with a person’s finger. For example, the person could raise or lower their finger to increase the strength of the lights in the room. If they wanted to play or pause the music in the house, they would simply tap or double tap the wall. This solution is low-cost and transforms very large surfaces into highly accurate human-computer interfaces that are specially tuned to gesture recognition.

(Harrison, 2008) out of Carnegie Mellon University proposed a similar type of device that could turn any surface into a high-accuracy touch interface. Although the research was limited, it opened up the possibility for “scratch input” to be a viable technique for human-computer interaction. With extension and application to a “smart home”, many possibilities exist for an inexpensive way to control lights, locks, and music without the need for a capacitive touch screen.</p>

<h1>Needfinding</h1>
<p>Participants would initially watch a Wizard of Oz prototype video first with no other information from the researchers about the product. They will then either take an online survey or be interviewed in person, using their initial reactions to the Wizard of Oz concept video.</p>

<h2>Survey Questions </h2>
<p>(Watch our concept video first)
Do you use Philips Hue lights? Would you use them in the future?
How likely would you be to use “this interface”.
Do you worry about privacy with microphones near you?
What applications do you see of this technology?
(Free response) For what purpose would you use a gesture technology like this? (i.e. controlling your lights, silencing your phone, controlling your computer)</p>

<h2>Interviews</h2>
<p>Have them watch the concept video. And then, tell us your immediate thoughts without (the researchers) telling you anything more about it. Free response, what do you think this product does? What other applications (not shown in the video) would you see being applied to with this technology? Would you use this product?
</p>

<h1>Prototyping</h1>
Wizard of Oz (The man behind the curtain)
Have volunteers come into the HCI lab and have them use the pre planned gestures
Play spotify songs / volume up / volume down
Have someone control the songs and the volume from a bluetooth controller with no one other than the volunteer in the room
Record and take live video of the user to see the reactions and test for facial expressions
Ask the user for feedback on the product, if they would use it again / buy the product
What other uses do they see for the product ?

This will give us a good understanding of how the users use the product and if there's a market for the device. Since the device is still in prototyping stage the final product can use the recommendations and feedback offered from the Wizard of Oz test.

<h1>Implementation</h1>

For implementation we plan to use a bluetooth microphone to record the sound waves created by the user's gestures, this gesture will be visible in real time on the computer. We plan to record the sounds created by different motions making sounds on different objects (ie. Table, walls, etc) and making a gesture out of them. Once these inputs are visualized and processed we plan to match them  to a pattern that we recognize as an input and find the following action associated to it.

We plan to implement several basic features such as increasing/decreasing brightness of lights, playing/pausing music, change the volume of music, navigating through menus.

Once we get to this point we plan to have a voice activated input with a gesture recognizer output. Somewhat like amazon Alexa where in when you say hey alexa it's activated and when it recognizes the gesture related to the input it reads it out aloud (ie. Playing music).

<h2>Our First few steps are:</h2>
Make the prototype
Get a Bluetooth microphone to read the noise created by a particular gesture
Visualize the inputs from the microphone in real time on a computer
Start to analyze gestures (develop the software)
Differentiate noise from gestures
The software will use the inputs to correspond it to a gesture
Connect with devices around you
Lights - Philips hue
Sound - Device Audio
Play Pause -  Spotify music
	Moving ahead (if time allows)
Learn trends and patterns in gestures  (Using Machine learning, learn each gesture and improve on errors)
Speak out the gesture being performed
Pausing music
Turning the brightness living room lights down
Possibly make this an all-in-one device so it has its own battery and doesn't need power supply
Have a intuitive mobile app to help set up devices and record custom input patterns

<h3>Things needed</h3>
<ul>
	<li>A bluetooth microphone </li>
	<li>Simulating software to display the wavelength of the sound waves being inputted </li>
	<li>Software to analyze the input by the device </li>
	<li>Getting the input to perform a gesture </li>
</ul>

<h1>Evaluation</h1>

Our product will be evaluated by users of all different ages and groups. People who understand recognize the importance of such gestures will be the ones that are most benefited. We plan to run trials on different groups of people to fully understand how each person will use our product. Once we find out what kind of people want to use the product, we will target these groups and expand our product thusly.

<h1>Timeline and deliverable</h1>
Concept video -  4th March
Do survey  - 12th March
Do interviews (response to concept video) - 25th March
Make prototype (wizard of oz system or other) - 1th April
Feedback on prototype (interviews, testing) - 8th April
Implement actual system (microphone, ML, gesture code). - Not yet decided
Evaluate actual system - Not yet decided

<h1>References</h1>
<ul>
	<li><a>http://chrisharrison.net/projects/scratchinput/ScratchInputHarrison.pdf</a></li>
	<li><a>http://www.chrisharrison.net/index.php/Research/ScratchInput</a></li>
	<li><a>https://www.amazon.com/dp/B011EXBCMQ?psc=1</a></li>
</ul>
</div>





<script src='http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js'></script>
<script src='http://masonry.desandro.com/masonry.pkgd.js'></script>
<script src="js/index.js"></script>
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
<!-- Optional theme -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css">
<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

</body>
</html>
